{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Text classification - classify if a news is fake or not (Use ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swapn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swapn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swapn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import  stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\swapn\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\swapn\\anaconda3\\lib\\site-packages (from wordcloud) (3.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\swapn\\anaconda3\\lib\\site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\swapn\\anaconda3\\lib\\site-packages (from wordcloud) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\swapn\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\swapn\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\swapn\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\swapn\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\swapn\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Tune in to the Alternate Current Radio Network...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Shawn Helton  21st Century WireWhen looking at...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Antifa (Photo: Twitter)Diana Johnstone 21st C...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>TWO PROTAGONISTS: Jesus Campos, and alleged sh...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>This latest move by America s notorious Transp...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  subject  fake\n",
       "0    Donald Trump just couldn t wish all Americans ...     News     1\n",
       "1    House Intelligence Committee Chairman Devin Nu...     News     1\n",
       "2    On Friday, it was revealed that former Milwauk...     News     1\n",
       "3    On Christmas day, Donald Trump announced that ...     News     1\n",
       "4    Pope Francis used his annual Christmas Day mes...     News     1\n",
       "..                                                 ...      ...   ...\n",
       "403  Tune in to the Alternate Current Radio Network...  US_News     0\n",
       "404  Shawn Helton  21st Century WireWhen looking at...  US_News     0\n",
       "405   Antifa (Photo: Twitter)Diana Johnstone 21st C...  US_News     0\n",
       "406  TWO PROTAGONISTS: Jesus Campos, and alleged sh...  US_News     0\n",
       "407  This latest move by America s notorious Transp...  US_News     0\n",
       "\n",
       "[408 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text content\n",
    "# 0. Convert data to lower/upper case\n",
    "# 1. creation of tokens - tokenization\n",
    "# 2. Remove Stopwords, punctuation marks, numbers\n",
    "# 3. Stemming or Lemmitization\n",
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  # FREE, Free, free\n",
    "  # Lower and tokenization\n",
    "  tokens = word_tokenize(text.lower())\n",
    "  # Filter only alphabets\n",
    "  word_tokens = [t for t in tokens if t.isalpha()]\n",
    "  # Remove stop words\n",
    "  clean_tokens = [t for t in word_tokens if t not in stop]\n",
    "  # Lemmitization\n",
    "  lemma = WordNetLemmatizer()\n",
    "  lemma_tokens = [lemma.lemmatize(t) for t in clean_tokens]\n",
    "  return \" \".join(lemma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello ABC #$%^#@@ 123 and is to words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello abc word'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text'] \n",
    "y = df['fake'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "403    0\n",
       "404    0\n",
       "405    0\n",
       "406    0\n",
       "407    0\n",
       "Name: fake, Length: 408, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    204\n",
       "0    204\n",
       "Name: fake, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    145\n",
       "1    140\n",
       "Name: fake, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    64\n",
       "0    59\n",
       "Name: fake, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorization\n",
    "cv = CountVectorizer()\n",
    "x_train_cv = cv.fit_transform(x_train)\n",
    "x_test_cv = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.24      0.21        59\n",
      "           1       0.06      0.05      0.05        64\n",
      "\n",
      "    accuracy                           0.14       123\n",
      "   macro avg       0.12      0.14      0.13       123\n",
      "weighted avg       0.12      0.14      0.13       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train_cv, y_train)\n",
    "y_pred = dt.predict(x_test_cv)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "x_train_tf = tf.fit_transform(x_train)\n",
    "x_test_tf = tf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.06063358,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.19      0.17        59\n",
      "           1       0.11      0.09      0.10        64\n",
      "\n",
      "    accuracy                           0.14       123\n",
      "   macro avg       0.14      0.14      0.14       123\n",
      "weighted avg       0.13      0.14      0.14       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train_tf, y_train)\n",
    "y_pred = dt.predict(x_test_tf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9507"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Ann you need to convert the x_train_cv and x_test_cv, x_train_tf and x_test_tf\n",
    "# into an array\n",
    "x_train_cv.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cv = x_train_cv.toarray()\n",
    "x_test_cv = x_test_cv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9507"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cv.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, activation=\"relu\", input_shape=(x_train_cv.shape[1], )))\n",
    "model.add(Dense(6, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 0.7221\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6747\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6638\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6428\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6398\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6199\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6174\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6219\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6067\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5987\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5839\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5767\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5664\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5543\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5567\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5427\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5440\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5403\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5275\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5457\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5363\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5374\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5292\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5172\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5201\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5163\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5109\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5342\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5544\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5438\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5155\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5092\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4998\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5145\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5133\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5204\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5140\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5164\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5119\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5301\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5205\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5009\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5222\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5139\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5157\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.502 - 0s 8ms/step - loss: 0.5009\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4927\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4983\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4912\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x240391abe80>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_cv, y_train, batch_size=70, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.79017979e-01],\n",
       "       [1.49106979e-02],\n",
       "       [3.79017979e-01],\n",
       "       [3.36206257e-02],\n",
       "       [2.30003595e-02],\n",
       "       [6.72600031e-01],\n",
       "       [7.26979971e-03],\n",
       "       [9.90286946e-01],\n",
       "       [1.07938856e-01],\n",
       "       [9.67122257e-01],\n",
       "       [4.51464593e-01],\n",
       "       [9.71855581e-01],\n",
       "       [5.39871573e-01],\n",
       "       [9.79553342e-01],\n",
       "       [4.18351889e-01],\n",
       "       [1.78017318e-02],\n",
       "       [9.98740971e-01],\n",
       "       [2.50308484e-01],\n",
       "       [9.62698460e-01],\n",
       "       [4.18351889e-01],\n",
       "       [2.30339170e-02],\n",
       "       [9.99999523e-01],\n",
       "       [3.05484653e-01],\n",
       "       [3.02553236e-01],\n",
       "       [6.81138039e-03],\n",
       "       [7.08379269e-01],\n",
       "       [4.75247622e-01],\n",
       "       [3.37976515e-01],\n",
       "       [9.97513413e-01],\n",
       "       [9.05670166e-01],\n",
       "       [4.20976400e-01],\n",
       "       [4.13601995e-02],\n",
       "       [5.92110455e-02],\n",
       "       [9.91621256e-01],\n",
       "       [5.55577636e-01],\n",
       "       [5.03507257e-03],\n",
       "       [7.60892153e-01],\n",
       "       [9.24551487e-03],\n",
       "       [5.63907623e-03],\n",
       "       [1.18625164e-03],\n",
       "       [2.94625759e-04],\n",
       "       [5.21609008e-01],\n",
       "       [8.78257275e-01],\n",
       "       [9.94373798e-01],\n",
       "       [9.91993129e-01],\n",
       "       [9.92007375e-01],\n",
       "       [1.50944591e-02],\n",
       "       [2.61149406e-02],\n",
       "       [9.74951446e-01],\n",
       "       [8.27260613e-02],\n",
       "       [8.58491659e-03],\n",
       "       [1.72678232e-02],\n",
       "       [9.40793753e-03],\n",
       "       [8.80342341e-05],\n",
       "       [5.96904755e-03],\n",
       "       [4.51464593e-01],\n",
       "       [7.60892153e-01],\n",
       "       [1.37618184e-02],\n",
       "       [9.99969661e-01],\n",
       "       [9.75891352e-01],\n",
       "       [6.42445683e-03],\n",
       "       [9.84247565e-01],\n",
       "       [4.57860827e-02],\n",
       "       [6.72600091e-01],\n",
       "       [5.21609008e-01],\n",
       "       [3.57031822e-03],\n",
       "       [5.39871573e-01],\n",
       "       [5.41934133e-01],\n",
       "       [4.35679048e-01],\n",
       "       [9.94339049e-01],\n",
       "       [9.96907711e-01],\n",
       "       [9.64491487e-01],\n",
       "       [1.13926828e-02],\n",
       "       [4.05083060e-01],\n",
       "       [9.56189036e-01],\n",
       "       [9.86259699e-01],\n",
       "       [4.05083060e-01],\n",
       "       [2.67025828e-03],\n",
       "       [2.20698118e-03],\n",
       "       [9.98908639e-01],\n",
       "       [4.75247622e-01],\n",
       "       [9.89927530e-01],\n",
       "       [2.76884437e-03],\n",
       "       [2.05470324e-02],\n",
       "       [9.75891352e-01],\n",
       "       [6.79105520e-04],\n",
       "       [9.82482016e-01],\n",
       "       [9.94716167e-01],\n",
       "       [8.45205784e-03],\n",
       "       [6.68257475e-04],\n",
       "       [9.99416709e-01],\n",
       "       [3.37976515e-01],\n",
       "       [9.94965136e-01],\n",
       "       [5.55577636e-01],\n",
       "       [8.93056393e-04],\n",
       "       [3.05484623e-01],\n",
       "       [9.99310493e-01],\n",
       "       [2.98814178e-02],\n",
       "       [9.75590050e-01],\n",
       "       [2.19339336e-06],\n",
       "       [7.08379269e-01],\n",
       "       [8.02130699e-01],\n",
       "       [9.91905689e-01],\n",
       "       [9.42526937e-01],\n",
       "       [5.41934133e-01],\n",
       "       [5.34084439e-03],\n",
       "       [9.88487542e-01],\n",
       "       [6.42074287e-01],\n",
       "       [4.35679048e-01],\n",
       "       [1.75011158e-03],\n",
       "       [9.82910514e-01],\n",
       "       [8.78257275e-01],\n",
       "       [5.11556864e-04],\n",
       "       [9.68787313e-01],\n",
       "       [1.15810335e-02],\n",
       "       [4.09004092e-03],\n",
       "       [6.21116161e-03],\n",
       "       [8.44673932e-01],\n",
       "       [9.77977157e-01],\n",
       "       [9.84734058e-01],\n",
       "       [5.03199697e-01],\n",
       "       [9.99467790e-01],\n",
       "       [9.74313796e-01]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.14      0.13        59\n",
      "           1       0.15      0.14      0.15        64\n",
      "\n",
      "    accuracy                           0.14       123\n",
      "   macro avg       0.14      0.14      0.14       123\n",
      "weighted avg       0.14      0.14      0.14       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.where(y_hat >= 0.5, 1, 0)\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf = x_train_tf.toarray()\n",
    "x_test_tf = x_test_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 4ms/step - loss: 0.6940\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6918\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6902\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6884\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6850\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6811\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6780\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6739\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6697\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6650\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6549\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6583\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6488\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6393\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6387\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6319\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6275\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6201\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6169\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5972\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5977\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5877\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5889\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5877\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5808\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5898\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5730\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5575\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5510\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5579\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5440\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5516\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5326\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5511\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5293\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5381\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5481\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5294\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5300\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5225\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5145\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5318\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5433\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5147\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5009\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5002\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5215\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4932\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5025\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24042e27eb0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(10, activation=\"relu\", input_shape=(x_train_cv.shape[1], )))\n",
    "model1.add(Dense(6, activation=\"relu\"))\n",
    "model1.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "model1.fit(x_train_tf, y_train, batch_size=60, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.14      0.13        59\n",
      "           1       0.15      0.14      0.15        64\n",
      "\n",
      "    accuracy                           0.14       123\n",
      "   macro avg       0.14      0.14      0.14       123\n",
      "weighted avg       0.14      0.14      0.14       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(x_test_tf)\n",
    "y_hat = np.where(y_hat >= 0.5, 1, 0)\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
